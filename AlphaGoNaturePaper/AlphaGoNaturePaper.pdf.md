# GPT-Academic Report
## # Title:





## # Abstract:





## # Meta Translation

标题：无

作者：D H Wrote

摘要：无

## # 

Mastering the game of Go with deep neural networks and tree search David Silver 1 *, Aja Huang 1 *, Chris J. Maddison 1 , Arthur Guez 1 , Laurent Sifre 1 , George van den Driessche 1 , Julian Schrittwieser 1 , Ioannis Antonoglou 1 , Veda Panneershelvam 1 , Marc Lanctot 1 , Sander Dieleman 1 , Dominik Grewe 1 , John Nham 2 , Nal Kalchbrenner 1 , Ilya Sutskever 2 , Timothy Lillicrap 1 , Madeleine Leach 1 , Koray Kavukcuoglu 1 , Thore Graepel 1 & Demis Hassabis 1 All games of perfect information have an optimal value function, v * (s), which determines the outcome of the game, from every board position or state s, under perfect play by all players. These games may be solved by recursively computing the optimal value function in a search tree containing approximately b d possible sequences of moves, where b is the game's breadth (number of legal moves per position) and d is its depth (game length). In large games, such as chess (b ≈ 35, d ≈ 80) 1 and especially Go (b ≈ 250, d ≈ 150) 1 , exhaustive search is infeasible 2,3 , but the effective search space can be reduced by two general principles. First, the depth of the search may be reduced by position evaluation: truncating the search tree at state s and replacing the subtree below s by an approximate value function v(s) ≈ v * (s) that predicts the outcome from state s. This approach has led to superhuman performance in chess 4 , checkers 5 and othello 6 , but it was believed to be intractable in Go due to the complexity of the game 7 . Second, the breadth of the search may be reduced by sampling actions from a policy p(a|s) that is a probability distribution over possible moves a in position s. For example, Monte Carlo rollouts 8 search to maximum depth without branching at all, by sampling long sequences of actions for both players from a policy p. Averaging over such rollouts can provide an effective position evaluation, achieving superhuman performance in backgammon 8 and Scrabble 9 , and weak amateur level play in Go 10 .
Monte Carlo tree search (MCTS) 11,12 uses Monte Carlo rollouts to estimate the value of each state in a search tree. As more simulations are executed, the search tree grows larger and the relevant values become more accurate. The policy used to select actions during search is also improved over time, by selecting children with higher values. Asymptotically, this policy converges to optimal play, and the evaluations converge to the optimal value function 12 . The strongest current Go programs are based on MCTS, enhanced by policies that are trained to predict human expert moves 13 . These policies are used to narrow the search to a beam of high-probability actions, and to sample actions during rollouts. This approach has achieved strong amateur play [13][14][15] . However, prior work has been limited to shallow policies [13][14][15] or value functions 16 based on a linear combination of input features.
Recently, deep convolutional neural networks have achieved unprecedented performance in visual domains: for example, image classification 17 , face recognition 18 , and playing Atari games 19 . They use many layers of neurons, each arranged in overlapping tiles, to construct increasingly abstract, localized representations of an image 20 . We employ a similar architecture for the game of Go. We pass in the board position as a 19 × 19 image and use convolutional layers to construct a representation of the position. We use these neural networks to reduce the effective depth and breadth of the search tree: evaluating positions using a value network, and sampling actions using a policy network.
We train the neural networks using a pipeline consisting of several stages of machine learning (Fig. 1). We begin by training a supervised learning (SL) policy network p σ directly from expert human moves. This provides fast, efficient learning updates with immediate feedback and high-quality gradients. Similar to prior work 13,15 , we also train a fast policy p π that can rapidly sample actions during rollouts. Next, we train a reinforcement learning (RL) policy network p ρ that improves the SL policy network by optimizing the final outcome of games of selfplay. This adjusts the policy towards the correct goal of winning games, rather than maximizing predictive accuracy. Finally, we train a value network v θ that predicts the winner of games played by the RL policy network against itself. Our program AlphaGo efficiently combines the policy and value networks with MCTS.

用深度神经网络和树搜索掌握围棋的游戏

所有的完全信息游戏都有一个最优值函数v*(s), 它确定了游戏的结果，从每一个棋盘位置或状态s，通过所有玩家的完美下棋。这些游戏可以通过递归计算搜索树中的最优值函数来解决，搜索树包含大约b^d个可能的移动序列，其中b是游戏的宽度（每个位置的合法移动数）和d是游戏的深度（游戏长度）。在大型游戏中，如国际象棋（b≈35，d≈80）和尤其是围棋（b≈250，d≈150），彻底搜索是不可行的，但有效的搜索空间可以通过两个一般原则来减少。首先，可以通过位置评估减少搜索的深度：在状态s处截断搜索树，并用一个近似值函数v(s)≈v*(s)替换s下方的子树，以预测状态s的结果。这种方法已经在国际象棋、桥牌和黑白棋中实现了超人类水平的表现，但由于围棋的复杂性，一直被认为是不可行的。其次，可以通过从在位置s上的可能移动a的概率分布p(a|s)中采样行动来减少搜索的广度。例如，蒙特卡洛模拟搜索可以在不分支的情况下搜索到最大深度，通过从策略p中为两个玩家抽样长序列的行动。通过对这样的模拟结果取平均值，可以提供有效的位置评估，在战舰游戏和Scrabble中实现超人类的表现，以及围棋中的低水平业余水平表现。

蒙特卡洛树搜索（MCTS）使用蒙特卡洛模拟搜索估计搜索树中每个状态的值。随着模拟次数的增加，搜索树变得越来越大，相关的值变得更加精确。在搜索过程中用于选择行动的策略也随时间改进，通过选择具有较高值的子节点。渐近地，这个策略收敛到最优解，评估收敛到最优值函数。目前最强的围棋程序基于MCTS，通过训练预测人类专家移动的策略进行增强。这些策略用于将搜索缩小到一束高概率行动，并在模拟中采样行动。这种方法在围棋中达到了强业余水平的表现。然而，先前的工作受到了浅策略或基于输入特征线性组合的值函数的限制。

最近，在视觉领域中深度卷积神经网络在图像分类、人脸识别和玩Atari游戏等方面取得了前所未有的性能。它们使用许多层的神经元，每个层都排列在重叠的图块中，以构建越来越抽象的本地化图像表示。我们在围棋游戏中采用了类似的架构。我们将棋盘位置传递给一个19×19的图像，并使用卷积层构建一个位置的表示。我们使用这些神经网络来减少搜索树的有效深度和广度：使用一个价值网络评估位置，使用一个策略网络采样行动。

我们使用一个多阶段机器学习的管道来训练神经网络（图1）。我们首先通过专家人类移动训练一个监督学习（SL）策略网络pσ。这提供了快速、高效的学习更新，具有即时反馈和高质量的梯度。与先前的工作类似，我们还训练了一个快速策略p π，可以在模拟中快速采样行动。接下来，我们通过自我对局的游戏的最终结果来改进SL策略网络的强化学习（RL）策略网络prho。这将策略调整为正确的目标-赢得游戏，而不是最大化预测准确性。最后，我们通过训练一个价值网络vtheta来预测RL策略网络相互对局的赢家。我们的AlphaGo程序高效地将策略和价值网络与MCTS结合起来。

## # Supervised learning of policy networks

For the first stage of the training pipeline, we build on prior work on predicting expert moves in the game of Go using supervised learning 13,[21][22][23][24] . The SL policy network p σ (a | s) alternates between convolutional layers with weights σ, and rectifier nonlinearities. A final softmax layer outputs a probability distribution over all legal moves a. The input s to the policy network is a simple representation of the board state (see Extended Data Table 2). The policy network is trained on randomly
The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of stateof-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.

在训练流程的第一阶段，我们借鉴了之前在预测围棋中专家走法方面的工作，使用监督学习算法[13] [21] [22] [23] [24]。SL策略网络pσ(a|s)在卷积层上交替使用权重σ和整流线性函数。最后一个softmax层输出一个关于所有合法走法a的概率分布。策略网络的输入s是对棋盘状态的简单表示（详见扩展数据表格2）。策略网络是在随机样本上进行训练的。

围棋长期以来一直被认为是人工智能面临的最具挑战性的经典游戏，因为它的搜索空间非常庞大，同时评估局面和棋步的难度也很大。在这里，我们介绍了一种新的计算机围棋方法，它使用“价值网络”评估局面，并使用“策略网络”选择走步。这些深度神经网络通过结合从人类专家对局的监督学习和自我对弈中的强化学习进行训练。在没有任何先行搜索的情况下，神经网络能够以与最先进的蒙特卡洛树搜索程序相当的水平进行围棋对弈，这些程序模拟了数千局随机对弈的情况。我们还引入了一种新的搜素算法，它将蒙特卡洛模拟与价值和策略网络结合起来。使用这个搜索算法，我们的AlphaGo程序在与其他围棋程序对弈中获胜率达到99.8%，并以5比0的成绩击败了人类的欧洲围棋冠军。这是计算机程序首次在完整尺寸的围棋游戏中战胜人类职业选手，这个成就原来被认为至少要在十年后才可能实现。

## # ARTICLE RESEARCH

sampled state-action pairs (s, a), using stochastic gradient ascent to maximize the likelihood of the human move a selected in state s
∆σ σ ∝ ∂ ( | ) ∂ σ p a s log
We trained a 13-layer policy network, which we call the SL policy network, from 30 million positions from the KGS Go Server. The network predicted expert moves on a held out test set with an accuracy of 57.0% using all input features, and 55.7% using only raw board position and move history as inputs, compared to the state-of-the-art from other research groups of 44.4% at date of submission 24 (full results in Extended Data Table 3). Small improvements in accuracy led to large improvements in playing strength (Fig. 2a); larger networks achieve better accuracy but are slower to evaluate during search. We also trained a faster but less accurate rollout policy p π (a|s), using a linear softmax of small pattern features (see Extended Data Table 4) with weights π; this achieved an accuracy of 24.2%, using just 2 μs to select an action, rather than 3 ms for the policy network.

我们以随机梯度上升的方式采样了状态动作对（s，a），以最大化人类在状态s中选择的移动a的可能性的目标函数。∆σ σ ∝ ∂ ( | ) ∂ σ p a s log。
我们训练了一个被称为SL策略网络的13层网络，来自KGS围棋服务器的3000万个位置。该网络在一个保留的测试集上预测专家级的移动，使用所有输入特征的准确率为57.0%，仅使用原始棋盘位置和移动历史作为输入的准确率为55.7%，而其他研究团队在提交日期24时的最新成果为44.4%（详见扩展数据表3）。准确率的小幅提升导致了下棋实力的大幅提升（图2a）；更大的网络能够取得更好的准确率，但在搜索过程中评估速度较慢。我们还训练了一个速度更快但准确率较低的推演策略p π (a|s)，使用小模式特征的线性softmax函数（见扩展数据表4）和权重π；该策略的准确率为24.2%，仅需2微秒选择一个动作，而策略网络则需要3毫秒。

## # Reinforcement learning of policy networks

The second stage of the training pipeline aims at improving the policy network by policy gradient reinforcement learning (RL) 25,26 . The RL policy network p ρ is identical in structure to the SL policy network, and its weights ρ are initialized to the same values, ρ = σ. We play games between the current policy network p ρ and a randomly selected previous iteration of the policy network. Randomizing from a pool of opponents in this way stabilizes training by preventing overfitting to the current policy. We use a reward function r(s) that is zero for all non-terminal time steps t < T. The outcome z t = ± r(s T ) is the terminal reward at the end of the game from the perspective of the current player at time step t: +1 for winning and -1 for losing. Weights are then updated at each time step t by stochastic gradient ascent in the direction that maximizes expected outcome 25
∆ρ ρ ∝ ∂ ( | ) ∂ ρ p a s z log t t t
We evaluated the performance of the RL policy network in game play, sampling each move ~(⋅| ) ρ a p s t t from its output probability distribution over actions. When played head-to-head, the RL policy network won more than 80% of games against the SL policy network. We also tested against the strongest open-source Go program, Pachi 14 , a sophisticated Monte Carlo search program, ranked at 2 amateur dan on KGS, that executes 100,000 simulations per move. Using no search at all, the RL policy network won 85% of games against Pachi. In comparison, the previous state-of-the-art, based only on supervised A reinforcement learning (RL) policy network p ρ is initialized to the SL policy network, and is then improved by policy gradient learning to maximize the outcome (that is, winning more games) against previous versions of the policy network. A new data set is generated by playing games of self-play with the RL policy network. Finally, a value network v θ is trained by regression to predict the expected outcome (that is, whether the current player wins) in positions from the self-play data set. b, Schematic representation of the neural network architecture used in AlphaGo. The policy network takes a representation of the board position s as its input, passes it through many convolutional layers with parameters σ (SL policy network) or ρ (RL policy network), and outputs a probability distribution ( | ) σ p a s or ( | ) ρ p a s over legal moves a, represented by a probability map over the board. The value network similarly uses many convolutional layers with parameters θ, but outputs a scalar value v θ (s′) that predicts the expected outcome in position s′. Positions and outcomes were sampled from human expert games. Each position was evaluated by a single forward pass of the value network v θ , or by the mean outcome of 100 rollouts, played out using either uniform random rollouts, the fast rollout policy p π , the SL policy network p σ or the RL policy network p ρ . The mean squared error between the predicted value and the actual game outcome is plotted against the stage of the game (how many moves had been played in the given position). 

训练流程的第二阶段旨在通过策略梯度强化学习（RL）来改善策略网络25,26. RL策略网络pρ在结构上与SL策略网络相同，其权重ρ初始化为相同的值ρ=σ。我们在当前策略网络pρ和随机选择的上一次迭代的策略网络之间进行游戏。通过从对手池中随机选择对手进行随机化，可以避免对当前策略的过度拟合，从而稳定训练。我们使用的奖励函数r(s)在所有非终止时间步t < T时均为零。结果zt=±r(sT)是当前玩家在时间步t结束时的终止奖励：+1表示获胜，-1表示失败。然后，在每个时间步t上，根据最大化预期结果的随机梯度上升更新权重25
∆ρρ∝ ∂ ( | ) ∂ρ p a sz logttt
我们通过从RL策略网络的输出概率分布中对每个动作采样每一步的下一步移动 ~(⋅| )ρapsstt 来评估RL策略网络的游戏性能。在面对面对局时，RL策略网络在与SL策略网络的对局中赢得了超过80%的比赛。我们还对其与最强的开源围棋程序Pachi14进行了测试，Pachi是一个复杂的蒙特卡洛搜索程序，在KGS上被评为2段业余段，每步执行100,000次模拟。在没有进行任何搜索的情况下，RL策略网络在与Pachi的比赛中赢得了85%的比赛。相比之下，仅基于监督A的最新技术水平，RL策略网络在与之前版本的策略网络对弈时能够最大化结果（即赢得更多比赛）。通过自我对弈与RL策略网络生成了一个新的数据集。最后，使用回归训练值网络vθ，以预测自我对弈数据集中位置的预期结果（即当前玩家是否获胜）。b, AlphaGo中使用的神经网络架构的示意图。策略网络以局面的表示s作为输入，经过多个带有参数σ（SL策略网络）或ρ（RL策略网络）的卷积层，并输出一个概率分布(|)σpas或(|)ρpas，表示合法移动a的概率图。值网络同样使用带有参数θ的多个卷积层，但输出一个标量值vθ(s')，用于预测位置s'中的预期结果。这些位置和结果是从人类专家对局中采样得到的。每个位置通过值网络vθ的单次前向传递评估，或通过100次模拟的平均结果进行评估，模拟可以使用均匀随机模拟、快速模拟策略pπ、SL策略网络pσ或RL策略网络pρ进行。预测值与实际对局结果之间的均方误差随游戏阶段（在给定位置下已经进行了多少步）绘制。

## # Reinforcement learning of value networks

The final stage of the training pipeline focuses on position evaluation, estimating a value function v p (s) that predicts the outcome from position s of games played by using policy p for both players [28][29][30] 
E ( ) = | = . v s z s s a p [ , ] p t t t T
Ideally, we would like to know the optimal value function under perfect play v * (s); in practice, we instead estimate the value function ρ v p for our strongest policy, using the RL policy network p ρ . We approximate the value function using a value network v θ (s) with weights θ, ⁎ ( ) ≈ ( ) ≈ ( )
θ ρ v s v s v s p
. This neural network has a similar architecture to the policy network, but outputs a single prediction instead of a probability distribution. We train the weights of the value network by regression on state-outcome pairs (s, z), using stochastic gradient descent to minimize the mean squared error (MSE) between the predicted value v θ (s), and the corresponding outcome z
∆θ θ ∝ ∂ ( ) ∂ ( -( )) θ θ v s z v s
The naive approach of predicting game outcomes from data consisting of complete games leads to overfitting. The problem is that successive positions are strongly correlated, differing by just one stone, but the regression target is shared for the entire game. When trained on the KGS data set in this way, the value network memorized the game outcomes rather than generalizing to new positions, achieving a minimum MSE of 0.37 on the test set, compared to 0.19 on the training set. To mitigate this problem, we generated a new self-play data set consisting of 30 million distinct positions, each sampled from a separate game. Each game was played between the RL policy network and itself until the game terminated. Training on this data set led to MSEs of 0.226 and 0.234 on the training and test set respectively, indicating minimal overfitting. Figure 2b shows the position evaluation accuracy of the value network, compared to Monte Carlo rollouts using the fast rollout policy p π ; the value function was consistently more accurate. A single evaluation of v θ (s) also approached the accuracy of Monte Carlo rollouts using the RL policy network p ρ , but using 15,000 times less computation.

训练流程的最后阶段专注于位置评估，即估计一个价值函数 v p (s)，该函数预测由玩家使用策略 p 进行对局的棋盘位置 s 的结果 [28][29][30] 
E ( ) = | = . v s z s s a p [ , ] p t t t T
理想情况下，我们希望知道完美对弈情况下的最佳价值函数 v * (s)；实际上，我们通过使用 RL 策略网络 p ρ 对我们最强大的策略进行估计值函数 ρ v p 。我们使用带有权重 θ 的值网络 v θ (s) 来近似价值函数，⁎ ( ) ≈ ( ) ≈ ( )
θ ρ v s v s v s p
。这个神经网络的结构与策略网络相似，但输出的是一个单一的预测值，而不是概率分布。我们通过对状态-结果对 (s, z) 进行回归来训练值网络的权重，使用随机梯度下降来最小化预测值 v θ (s) 与对应结果 z 之间的均方误差 (MSE)
∆θ θ ∝ ∂ ( ) ∂ ( -( )) θ θ v s z v s
从由完整对局数据预测对局结果的朴素方法会导致过拟合。问题在于连续的位置之间强相关，仅相差一颗棋子，但回归目标应用于整个对局。当采用这种方式训练KGS数据集时，值网络会记住对局结果而不是推广到新的位置，测试集上的最小均方误差为0.37，而训练集上为0.19。为了缓解这个问题，我们生成了一个新的自动对局数据集，包含 3000 万个不同的棋盘位置，其中每个位置从单独的对局中采样得到。每个对局是由 RL 策略网络与其自身进行对弈，直到对局结束。在此数据集上进行训练的结果为训练集和测试集上的均方误差分别为 0.226 和 0.234，表示了最小的过拟合。图2b展示了与使用快速展开策略 p π 的蒙特卡洛模拟对比的值网络的位置评估准确度；值函数始终更准确。通过一个对 v θ (s) 的评估，准确度也接近于使用 RL 策略网络 p ρ 的蒙特卡洛模拟，但计算量减少了 15000 倍。

## # Searching with policy and value networks

AlphaGo combines the policy and value networks in an MCTS algorithm (Fig. 3) that selects actions by lookahead search. Each edge (s, a) of the search tree stores an action value Q(s, a), visit count N(s, a), and prior probability P(s, a). The tree is traversed by simulation (that is, descending the tree in complete games without backup), starting from the root state. that is proportional to the prior probability but decays with repeated visits to encourage exploration. When the traversal reaches a leaf node s L at step L, the leaf node may be expanded. The leaf position s L is processed just once by the SL policy network p σ . The output probabilities are stored as prior probabilities P for each legal action a,
( )= ( | ) σ P s a p a s ,
. The leaf node is evaluated in two very different ways: first, by the value network v θ (s L ); and second, by the outcome z L of a random rollout played out until terminal step T using the fast rollout policy p π ; these evaluations are combined, using a mixing parameter λ, into a leaf evaluation V(s L )
λ λ ( )= ( -) ( )+ θ V s v s z 1 L L L
At the end of simulation, the action values and visit counts of all traversed edges are updated. Each edge accumulates the visit count and mean evaluation of all simulations passing through that edge
∑ ∑ ( )= ( ) ( )= ( ) ( ) ( ) = = N s a sa i Q s a N s a s a i V s , 1 , ,, 1 , 1 ,
,
i n i n L i 1 1
where s L i is the leaf node from the ith simulation, and 1(s, a, i) indicates whether an edge (s, a) was traversed during the ith simulation. Once the search is complete, the algorithm chooses the most visited move from the root position.
It is worth noting that the SL policy network p σ performed better in AlphaGo than the stronger RL policy network p ρ , presumably because humans select a diverse beam of promising moves, whereas RL optimizes for the single best move. However, the value function
( ) ≈ ( ) θ ρ v s v s p
derived from the stronger RL policy network performed 

AlphaGo将策略网络和价值网络结合在一个MCTS算法中（图3），通过前瞻搜索选择动作。搜索树的每一条边（s, a）都存储着动作值Q(s, a)、访问计数N(s, a)和先验概率P(s, a)。树是通过模拟来遍历的（也就是，从根状态开始，沿着树下降进行完整游戏，不进行备份）。当遍历到第L步时，如果遍历到一个叶节点s L ，则可以对该叶节点进行扩展。叶位置s L 只会被SL策略网络p σ 处理一次。输出概率被存储为每个合法动作a的先验概率P，
( )= ( | ) σ P s a p a s ,
。叶节点会经过两种不同的方式进行评估：首先，通过值网络v θ (s L )进行评估；其次，通过随机模拟游戏直到终止步T，利用快速模拟策略p π 产生的结果z L 进行评估；这些评估会通过混合参数λ进行合并，形成一个叶节点的评估值V(s L )，
λ λ ( )= ( -) ( )+ θ V s v s z 1 L L L
在模拟结束时，所有遍历过的边的动作值和访问计数都会更新。每一条边都会累加所有通过该边的模拟的访问计数和平均评估值，
∑ ∑ ( )= ( ) ( )= ( ) ( ) ( ) = = N s a sa i Q s a N s a s a i V s , 1 , ,, 1 , 1 ,
,
i n i n L i 1 1
其中s L i是第i次模拟的叶节点，并且1(s, a, i)表示在第i次模拟中是否遍历了边（s, a）。一旦搜索完成，算法将选择从根位置中访问最多的动作。

值得注意的是，在AlphaGo中，SL策略网络p σ 的表现比更强的RL策略网络p ρ 要好，这可能是因为人类会选择多种有前途的动作，而RL则优化了单个最佳动作。然而，从更强的RL策略网络派生出来的值函数
( ) ≈ ( ) θ ρ v s v s p
在AlphaGo中表现良好。

## # Evaluating the playing strength of AlphaGo

To evaluate AlphaGo, we ran an internal tournament among variants of AlphaGo and several other Go programs, including the strongest commercial programs Crazy Stone 13 and Zen, and the strongest open source programs Pachi 14 and Fuego 15 . All of these programs are based  67891011). Each program used approximately 5 s computation time per move. To provide a greater challenge to AlphaGo, some programs (pale upper bars) were given four handicap stones (that is, free moves at the start of every game) against all opponents. Programs were evaluated on an Elo scale 37 : a 230 point gap corresponds to a 79% probability of winning, which roughly corresponds to one amateur dan rank advantage on KGS 38 ; an approximate correspondence to human ranks is also shown, horizontal lines show KGS ranks achieved online by that program. Games against the human European champion Fan Hui were also included; these games used longer time controls. 95% confidence intervals are shown. b, Performance of AlphaGo, on a single machine, for different combinations of components. The version solely using the policy network does not perform any search. c, Scalability study of MCTS in AlphaGo with search threads and GPUs, using asynchronous search (light blue) or distributed search (dark blue), for 2 s per move.  The results of the tournament (see Fig. 4a) suggest that singlemachine AlphaGo is many dan ranks stronger than any previous Go program, winning 494 out of 495 games (99.8%) against other Go programs. To provide a greater challenge to AlphaGo, we also played games with four handicap stones (that is, free moves for the opponent); AlphaGo won 77%, 86%, and 99% of handicap games against Crazy Stone, Zen and Pachi, respectively. The distributed version of AlphaGo was significantly stronger, winning 77% of games against single-machine AlphaGo and 100% of its games against other programs.
We also assessed variants of AlphaGo that evaluated positions using just the value network (λ = 0) or just rollouts (λ = 1) (see Fig. 4b). Even without rollouts AlphaGo exceeded the performance of all other Go programs, demonstrating that value networks provide a viable alternative to Monte Carlo evaluation in Go. However, the mixed evaluation (λ = 0.5) performed best, winning ≥95% of games against other variants. This suggests that the two position-evaluation mechanisms are complementary: the value network approximates the outcome of games played by the strong but impractically slow p ρ , while the rollouts can precisely score and evaluate the outcome of games played by the weaker but faster rollout policy p π . Figure 5 visualizes the evaluation of a real game position by AlphaGo.
Finally, we evaluated the distributed version of AlphaGo against Fan Hui, a professional 2 dan, and the winner of the 2013, 2014 and 2015 European Go championships. Over 5-9 October 2015 AlphaGo and Fan Hui competed in a formal five-game match. AlphaGo won the match 5 games to 0 (Fig. 6 and Extended Data Table 1). This is the first time that a computer Go program has defeated a human professional player, without handicap, in the full game of Go-a feat that was previously believed to be at least a decade away 3,7,31 .

为了对AlphaGo进行评估，我们在AlphaGo的几个变种以及其他一些围棋程序之间进行了内部锦标赛，包括最强的商业程序Crazy Stone 13和Zen，以及最强的开源程序Pachi 14和Fuego 15。所有这些程序都基于Monte Carlo树搜索（MCTS）算法。每个程序每步使用的计算时间约为5秒。为了对AlphaGo提供更大的挑战，一些程序（上方淡色柱状条）在与所有对手的比赛中获得了四个定石（即在每局游戏开始时的免费着法）。程序的评估是基于Elo等级。230个等级的差距对应着79%的获胜概率，这大致相当于在KGS上的业余段位优势；水平线显示了该程序在KGS上在线达到的人类段位。还包括与欧洲冠军樊麾的对局；这些对局使用了更长的时间控制。显示了95%的置信区间。b，AlphaGo在不同组件组合下的性能，单纯使用策略网络的版本不进行搜索。c，AlphaGo中使用搜索线程和GPU的MCTS的可扩展性研究，使用异步搜索（浅蓝色）或分布式搜索（深蓝色），每步2秒。锦标赛的结果（参见图4a）表明，单机版AlphaGo比以前的任何围棋程序都要强数段，与其他围棋程序相比，赢得了495场比赛中的494场（99.8%）。为了对AlphaGo提供更大的挑战，我们还进行了四个定石规则下的对局（即对手的免费着法）；AlphaGo分别赢得了Crazy Stone、Zen和Pachi的77%、86%和99%的定石对局。分布式版的AlphaGo更加强大，对单机版AlphaGo赢得了77%的比赛，并对其他程序赢得了100%的比赛。

我们还评估了只使用价值网络（λ = 0）或仅使用滚动（λ = 1）来评估位置的AlphaGo的变体（参见图4b）。即使在没有滚动的情况下，AlphaGo的性能仍超过了所有其他围棋程序，表明在围棋中价值网络提供了一个可行的替代方法来进行Monte Carlo评估。然而，混合评估（λ = 0.5）表现最好，在与其他变体的比赛中赢得了至少95%的比赛。这表明两种位置评估机制是互补的：价值网络近似评估了由强但速度较慢的策略网络p ρ 执行的游戏的结果，而滚动则可以精确评分和评估由较弱但速度较快的滚动策略p π 执行的游戏的结果。图5显示了AlphaGo对一个真实游戏局面的评估。

最后，我们对分布式版的AlphaGo进行了与职业2段、2013年、2014年和2015年欧洲围棋冠军樊麾的对局进行评估。在2015年10月5日至9日，AlphaGo与樊麾进行了正式的五局对局。AlphaGo以5比0赢得了比赛（图6和扩展数据表1）。这是计算机围棋程序首次在没有定石优势的情况下战胜职业选手，这被认为至少还需要十年的时间才可能实现。

## # Discussion

In this work we have developed a Go program, based on a combination of deep neural networks and tree search, that plays at the level of the strongest human players, thereby achieving one of artificial intelligence's "grand challenges" [31][32][33] . We have developed, for the first time, effective move selection and position evaluation functions for Go, based on deep neural networks that are trained by a novel combination   During the match against Fan Hui, AlphaGo evaluated thousands of times fewer positions than Deep Blue did in its chess match against Kasparov 4 ; compensating by selecting those positions more intelligently, using the policy network, and evaluating them more precisely, using the value network-an approach that is perhaps closer to how humans play. Furthermore, while Deep Blue relied on a handcrafted evaluation function, the neural networks of AlphaGo are trained directly from gameplay purely through general-purpose supervised and reinforcement learning methods.
Go is exemplary in many ways of the difficulties faced by artificial intelligence 33,34 : a challenging decision-making task, an intractable search space, and an optimal solution so complex it appears infeasible to directly approximate using a policy or value function. The previous major breakthrough in computer Go, the introduction of MCTS, led to corresponding advances in many other domains; for example, general game-playing, classical planning, partially observed planning, scheduling, and constraint satisfaction 35,36 . By combining tree search with policy and value networks, AlphaGo has finally reached a professional level in Go, providing hope that human-level performance can now be achieved in other seemingly intractable artificial intelligence domains.
Online Content Methods, along with any additional Extended Data display items and Source Data, are available in the online version of the paper; references unique to these sections appear only in the online paper. 

在这项工作中，我们开发了一个基于深度神经网络和树搜索的围棋程序，其水平达到了最强人类选手的水平，从而实现了人工智能的一个“重大挑战”[31][32][33]。我们首次开发了针对围棋的有效移动选择和位置评估函数，这些函数基于通过一种新颖的组合进行训练的深度神经网络。在与Fan Hui的比赛中，AlphaGo评估的位置数量比Deep Blue在对抗Kasparov的国际象棋比赛中少了数千倍；通过使用策略网络更加智能地选择这些位置，并且使用值网络更精确地评估这些位置来进行补偿，这种方法或许更接近人类的下棋方式。此外，Deep Blue依赖于手工设计的评估函数，而AlphaGo的神经网络则是通过通用的监督学习和增强学习方法直接从对局中进行训练的。

从许多方面来看，围棋是人工智能面临的困难的典型案例[33,34]：一个具有挑战性的决策任务，一个难以处理的搜索空间，以及一个复杂到似乎无法通过策略或值函数直接逼近的最优解。前一次在计算机围棋中的重大突破，即蒙特卡洛树搜索的引入，导致了许多其他领域的相应进展。例如，通用游戏玩法、经典规划、部分可观察规划、调度和约束满足等[35,36]。通过将树搜索与策略和值网络相结合，AlphaGo最终在围棋中达到了职业水平，为人类级别的表现提供了希望，同时也为其他看似难以处理的人工智能领域的实现提供了指引。

在线版本的论文中提供了在线内容方法，以及任何附加的扩展数据显示项和源数据。这些部分独有的参考文献仅出现在在线论文中。

## # METHODS Part-1

Problem setting. Many games of perfect information, such as chess, checkers, othello, backgammon and Go, may be defined as alternating Markov games 39 . In these games, there is a state space S (where state includes an indication of the current player to play); an action space ( ) A s defining the legal actions in any given state s ∈ S; a state transition function f(s, a, ξ) defining the successor state after selecting action a in state s and random input ξ (for example, dice); and finally a reward function r i (s) describing the reward received by player i in state s. We restrict our attention to two-player zero-sum games, r 1 (s) = -r 2 (s) = r(s), with deterministic state transitions, f(s, a, ξ) = f(s, a), and zero rewards except at a terminal time step T. The outcome of the game z t = ±r(s T ) is the terminal reward at the end of the game from the perspective of the current player at time step t. A policy p(a|s) is a probability distribution over legal actions a ∈ ( ) A s . A value function is the expected outcome if all actions for both players are selected according to policy p, that is,
E ( ) = | = ... v s z s s a p [ , ] p t t t T
. Zero-sum games have a unique optimal value function v*(s) that determines the outcome from state s following perfect play by both players,
⁎ ⁎ ( ) =        = -( ( )) v s z ss v f s a if , max
, otherwise
T T a
Prior work. The optimal value function can be computed recursively by minimax (or equivalently negamax) search 40 . Most games are too large for exhaustive minimax tree search; instead, the game is truncated by using an approximate value function v(s) ≈ v * (s) in place of terminal rewards. Depth-first minimax search with alpha-beta pruning 40 has achieved superhuman performance in chess 4 , checkers 5 and othello 6 , but it has not been effective in Go 7 .
Reinforcement learning can learn to approximate the optimal value function directly from games of self-play 39 . The majority of prior work has focused on a linear combination v θ (s) = ϕ(s) • θ of features ϕ(s) with weights θ. Weights were trained using temporal-difference learning 41 in chess 42,43 , checkers 44,45 and Go 30 ; or using linear regression in othello 6 and Scrabble 9 . Temporal-difference learning has also been used to train a neural network to approximate the optimal value function, achieving superhuman performance in backgammon 46 ; and achieving weak kyu-level performance in small-board Go 28,29,47 using convolutional networks.
An alternative approach to minimax search is Monte Carlo tree search (MCTS) 11,12 , which estimates the optimal value of interior nodes by a double approximation,
⁎ ( ) ≈ ( ) ≈ ( ) V s v s v s n P n
. The first approximation, ( ) ≈ ( ) V s v s n P n , uses n Monte Carlo simulations to estimate the value function of a simulation policy P n . The second approximation,
⁎ ( ) ≈ ( ) v s v s P n

问题设置。许多完全信息游戏，如国际象棋、跳棋、黑白棋、双陆棋和围棋，可定义为交替马尔科夫游戏39。在这些游戏中，有一个状态空间S（其中状态包括当前玩家的指示）；一个动作空间As，定义在任意给定状态s ∈ S中合法的动作；一个状态转换函数f(s, a, ξ)，定义在状态s中选择动作a后的继任状态，并且包含随机输入ξ（例如，骰子）；最后是一个奖励函数ri(s)，描述了玩家i在状态s中获得的奖励。我们限制我们的注意力在两人零和游戏上，ri(s) = -r2(s) = r(s)，具有确定性状态转换f(s, a, ξ) = f(s, a)，并且在终止时间步T只有零奖励。游戏的结果zt = ±r(sT)是当前玩家在时间步t的观点下游戏结束时的终止奖励。策略p(a|s)是在合法动作a ∈ As上的概率分布。值函数是如果根据策略p选择了双方玩家的所有动作，则期望的结果，即
E(s)=|t=0,1,2,...Tvst Γp(at|st)
。零和游戏有一个唯一的最优值函数v*(s)，它通过双方完全的博弈来确定从状态s开始的结果，
⁎(s)= −maxa,f(s,a)r(s,a)+v(f(s,a))ifT,otherwise
在先前的工作中，最优值函数可以通过极小化极大（或等价的负极小）搜索来递归地计算40。大多数游戏的规模太大了，无法进行全面的极小化树搜索，相反，通过使用近似值函数v(s) ≈ v*（s）来替代终止奖励来截断游戏。使用alpha-beta剪枝的深度优先极小化搜索40在国际象棋4、跳棋5和黑白棋6方面取得了超人类的表现，但在围棋7方面并不有效。强化学习可以直接从自我对弈的游戏中学习逼近最优值函数39。大多数先前的工作着重于特征ϕ(s)和权重θ的线性组合vθ(s) = ϕ(s) • θ。在国际象棋42,43、跳棋44,45和围棋30方面，用于训练权重的是时序差分学习41；在黑白棋6和Scrabble 9方面，用于训练的是线性回归。时序差分学习也被用来训练一个神经网络来逼近最优值函数，在双陆棋46方面取得了超人类的表现，在小棋局的围棋28,29,47中使用卷积神经网络取得了弱九段水平的表现。另一种极小化树搜索的方法是蒙特卡罗树搜索（MCTS）11,12，它通过双重逼近来估计内部节点的最优值，
⁎(s) ≈ (s) ≈ (s)V(s) v(s)P(n)
第一次逼近，(s) ≈ (s)V(s)v(s)P(n)，使用n次蒙特卡罗模拟来估计模拟策略P(n)的值函数。第二次逼近，
⁎(s) ≈ (s)v(s)P(n)

## # METHODS Part-2

, uses a simulation policy P n in place of minimax optimal actions. The simulation policy selects actions according to a search control function
( ( ) + ( )) Q s a u s a argmax , ,a n
, such as UCT 12 , that selects children with higher action values, Q n (s, a) = -V n (f(s, a)), plus a bonus u(s, a) that encourages exploration; or in the absence of a search tree at state s, it samples actions from a fast rollout policy ( | ) π p a s . As more simulations are executed and the search tree grows deeper, the simulation policy becomes informed by increasingly accurate statistics. In the limit, both approximations become exact and MCTS (for example, with UCT) converges 12 to the optimal value function
⁎ ( ) = () = ( ) →∞ →∞ V s v s v s lim lim n n n P n
. The strongest current Go programs are based on MCTS [13][14][15]36 .
MCTS has previously been combined with a policy that is used to narrow the beam of the search tree to high-probability moves 13 ; or to bias the bonus term towards high-probability moves 48 . MCTS has also been combined with a value function that is used to initialize action values in newly expanded nodes 16 , or to mix Monte Carlo evaluation with minimax evaluation 49 . By contrast, AlphaGo's use of value functions is based on truncated Monte Carlo search algorithms 8,9 , which terminate rollouts before the end of the game and use a value function in place of the terminal reward. AlphaGo's position evaluation mixes full rollouts with truncated rollouts, resembling in some respects the well-known temporal-difference learning algorithm TD(λ). AlphaGo also differs from prior work by using slower but more powerful representations of the policy and value function; evaluating deep neural networks is several orders of magnitude slower than linear representations and must therefore occur asynchronously.
The performance of MCTS is to a large degree determined by the quality of the rollout policy. Prior work has focused on handcrafted patterns 50 or learning rollout policies by supervised learning 13 , reinforcement learning 16 , simulation balancing 51,52 or online adaptation 30,53 ; however, it is known that rollout-based position evaluation is frequently inaccurate 54 . AlphaGo uses relatively simple rollouts, and instead addresses the challenging problem of position evaluation more directly using value networks.
Search algorithm. To efficiently integrate large neural networks into AlphaGo, we implemented an asynchronous policy and value MCTS algorithm (APV-MCTS). Each node s in the search tree contains edges (s, a) for all legal actions a ∈ ( ) A s . Each edge stores a set of statistics, The APV-MCTS algorithm proceeds in the four stages outlined in Fig. 3. Selection (Fig. 3a). The first in-tree phase of each simulation begins at the root of the search tree and finishes when the simulation reaches a leaf node at time step L. At each of these time steps, t < L, an action is selected according to the statistics in the search tree, = ( (  3d). At each in-tree step t ≤ L of the simulation, the rollout statistics are updated as if it has lost n vl games, N r (s t , a t ) ← N r (s t , a t ) + n vl ; W r (s t , a t ) ← W r (s t , a t ) -n vl ; this virtual loss 55 discourages other threads from simultaneously exploring the identical variation. At the end of the simulation, t he rollout statistics are updated in a backward pass through each step t ≤ L, replacing the virtual losses by the outcome, N r (s t , a t ) ← N r (s t , a t ) -n vl + 1; W r (s t , a t ) ← W r (s t , a t ) + n vl + z t . Asynchronously, a separate backward pass is initiated when the evaluation of the leaf position s L completes. The output of the value network v θ (s L ) is used to update value statistics in a second backward pass through each step t ≤ L, N v (s t , a t ) ← N v (s t , a t ) + 1, W v (s t , a t ) ← W v (s t , a t ) + v θ (s L ). The overall evaluation of each state action is a weighted average of the Monte Carlo estimates,
( ) ( ) ( ) ( ) ( ) ( ) P s a N s a N s a W s a W s a Q s a { , , ,, , , , , , , , } v

, 将最小最大优化动作替换为模拟策略Pn。模拟策略根据搜索控制函数（(()) + (())) Q_s_a_u_s_a_argmax，例如UCT 12，选择具有较高行动值的子节点，Q_n(s, a) = -V_n(f(s, a))，加上鼓励探索的奖励u(s, a)；或者在状态s处没有搜索树时，从快速模拟策略(|)π_p_a_s中随机选择动作。随着执行更多的模拟并且搜索树变得更深，模拟策略逐渐由越来越准确的统计信息指导。在极限情况下，两种近似变得精确，MCTS（例如，使用UCT）收敛到最优值函数，即"*V_s=lim_n→∞P_n(*)"。目前最强大的围棋程序基于MCTS[13][14][15]36。在先前的研究中，MCTS曾与用于缩小搜索树搜索范围的策略相结合，选择高概率的移动[13]；或者将奖励项偏向高概率的移动[48]。MCTS还与用于初始化新扩展节点中的动作值的值函数相结合[16]，或者将蒙特卡洛评估与最小最大评估相结合[49]。相比之下，AlphaGo使用值函数是基于截断蒙特卡洛搜索算法[8,9]，在游戏结束之前终止模拟并使用值函数代替终端奖励。AlphaGo对位置评估的方法是将完整的模拟和截断模拟相结合，与广为人知的时间差分学习算法TD(λ)在某些方面相似。AlphaGo还与之前的工作不同，它使用较慢但更强大的策略和值函数表示方法；评估深度神经网络比线性表示慢几个数量级，因此必须异步地进行。

MCTS的性能在很大程度上取决于模拟策略的质量。以前的工作侧重于手工制作的模式[50]，或通过监督学习[13]、强化学习[16]、模拟平衡[51,52]或在线适应[30,53]来学习模拟策略；然而，众所周知，基于模拟的位置评估经常不准确[54]。AlphaGo使用相对简单的模拟策略，而更直接地解决了位置评估这个具有挑战性的问题，使用了值网络。

搜索算法。为了将大型神经网络高效地整合到AlphaGo中，我们实现了一种异步策略和值MCTS算法（APV-MCTS）。搜索树中的每个节点s包含所有合法动作a∈(A_s)的边缘（s, a）。每个边缘存储一组统计信息。APV-MCTS算法按照图3中概述的四个阶段进行。选择阶段（图3a）。每次模拟的第一个在树内阶段从搜索树的根部开始，并在模拟达到叶节点的时间步长L结束。在这些时间步长中，t < L，根据搜索树中的统计信息选择动作，= (3d)。在模拟的每个树内步骤t ≤ L中，更新模拟统计信息，就好像它输掉了n_vl场比赛，N_r(s_t, a_t) ← N_r(s_t, a_t) + n_vl；W_r(s_t, a_t) ← W_r(s_t, a_t) - n_vl；这个虚拟损失(55)会阻止其他线程同时探索相同的变化。在模拟结束时，通过每个步骤t ≤ L的反向传播，用结果替换虚拟损失，N_r(s_t, a_t) ← N_r(s_t, a_t) - n_vl + 1；W_r(s_t, a_t) ← W_r(s_t, a_t) + n_vl + z_t。同时，当叶子位置s_L的评估完成时，启动单独的反向传播。值网络v_θ(s_L)的输出用于通过每个步骤t ≤ L的第二次反向传播来更新值统计信息，N_v(s_t, a_t) ← N_v(s_t, a_t) + 1；W_v(s_t, a_t) ← W_v(s_t, a_t) + v_θ(s_L)。每个状态动作的整体评估是蒙特卡洛估计的加权平均值，(P_s_a) = (N_s_a)/(N_s)，其中，(N_s)是状态s的访问次数。最后，选择具有最大行动值的动作作为最佳动作。

## # METHODS Part-3

)+ ( )) a Q s a u s
λ λ ( )= ( -) + ( ) ( ) ( ) ( ) Q s a , 1 W s a N s a W s a N s a , ,, , v v r r
, that mixes together the value network and rollout evaluations with weighting parameter λ. All updates are performed lock-free 56 . Expansion (Fig. 3b). When the visit count exceeds a threshold, N r (s, a) > n thr , the successor state s′ = f(s, a) is added to the search tree. The new node is initialized to {N(s′, a) = N r (s′, a) = 0, W(s′, a) = W r (s′, a) = 0, P(s′,a) = p σ (a|s′)}, using a tree policy p τ (a|s′) (similar to the rollout policy but with more features, see Extended Data Table 4) to provide placeholder prior probabilities for action selection. The position s′ is also inserted into a queue for asynchronous GPU evaluation by the policy network. Prior probabilities are computed by the SL policy network (⋅| ′) σ β p s with a softmax temperature set to β; these replace the placeholder prior probabilities, ( ′ ) ← ( | ′) σ β P s a p a s , , using an atomic update. The threshold n thr is adjusted dynamically to ensure that the rate at which positions are added to the policy queue matches the rate at which the GPUs evaluate the policy network. Positions are evaluated by both the policy network and the value network using a mini-batch size of 1 to minimize end-to-end evaluation time.
We also implemented a distributed APV-MCTS algorithm. This architecture consists of a single master machine that executes the main search, many remote worker CPUs that execute asynchronous rollouts, and many remote worker GPUs that execute asynchronous policy and value network evaluations. The entire search tree is stored on the master, which only executes the in-tree phase of each simulation. The leaf positions are communicated to the worker CPUs, which execute the rollout phase of simulation, and to the worker GPUs, which compute network features and evaluate the policy and value networks. The prior probabilities of the policy network are returned to the master, where they replace placeholder prior probabilities at the newly expanded node. The rewards from rollouts and the value network outputs are each returned to the master, and backed up the originating search path.
At the end of search AlphaGo selects the action with maximum visit count; this is less sensitive to outliers than maximizing action value 15 . The search tree is reused at subsequent time steps: the child node corresponding to the played action becomes the new root node; the subtree below this child is retained along with all its statistics, while the remainder of the tree is discarded. The match version of AlphaGo continues searching during the opponent's move. It extends the search

)+ ( )) a Q s a u s
λ λ ( )= ( -) + ( ) ( ) ( ) ( ) Q s a , 1 W s a N s a W s a N s a , ,, , v v r r
该方法结合了值网络和模拟评估，并使用加权参数λ进行混合。所有的更新都是无锁操作[56]。扩展（图3b）。当访问计数超过阈值时，N(s, a) > n_thr, 将后继状态s' = f(s, a)添加到搜索树中。新节点的初始化是{N(s', a) = N_r (s', a) = 0, W(s', a) = W_r (s', a) = 0, P(s',a) = p_τ (a|s')}，其中p_τ (a|s')是一种树策略（类似于模拟策略，但具有更多的特征，参见扩展数据表4），用于为行动选择提供占位符的先验概率。位置s' 也被插入异步GPU进行策略网络评估的队列中。先验概率由SL策略网络计算得到(⋅| ′) σ β p(s|a) ，其中σ是以β为参数的softmax函数。这些先验概率替换掉占位符较低的先验概率( ′ ) ← ( | ′) σ β P(s|a) ，使用原子更新操作。阈值n_thr动态调整，以确保添加到策略队列中的位置的速率与GPU评估策略网络的速率相匹配。使用mini-batch大小为1来评估策略网络和值网络以最小化端到端的评估时间。

我们还实现了分布式的APV-MCTS算法。该架构由一个执行主搜索的主机器、多个远程工作CPU执行异步模拟，以及多个远程工作GPU执行异步策略和值网络评估组成。整个搜索树存储在主机器上，只执行每次模拟的树内阶段。叶子位置被传递给工作CPU，它们执行模拟的扩展阶段，并传递给工作GPU，进行网络特征的计算和策略与值网络的评估。策略网络的先验概率返回给主机器，在新扩展的节点上替换掉占位符的先验概率。模拟的奖励和值网络的输出分别返回给主机器，并备份原始的搜索路径。

在搜索结束时，AlphaGo选择访问计数最大的动作；这比最大化动作值对异常值不敏感[15]。搜索树在后续时间步骤中被重复使用：对于已经选择的行动，其对应的子节点成为新的根节点；该子节点下的子树及其所有统计信息被保留，而其余部分被丢弃。AlphaGo的比赛版本在对手的回合中继续搜索。它扩展搜索

## # ARTICLE RESEARCH Part-1

if the action maximizing visit count and the action maximizing action value disagree. Time controls were otherwise shaped to use most time in the middle-game 57 . AlphaGo resigns when its overall evaluation drops below an estimated 10% probability of winning the game, that is, ( )< -. Q s a max , 08 a . AlphaGo does not employ the all-moves-as-first 10 or rapid action value estimation 58 heuristics used in the majority of Monte Carlo Go programs; when using policy networks as prior knowledge, these biased heuristics do not appear to give any additional benefit. In addition AlphaGo does not use progressive widening 13 , dynamic komi 59 or an opening book 60 . The parameters used by AlphaGo in the Fan Hui match are listed in Extended Data Table 5. Rollout policy. The rollout policy ( | ) π p a s is a linear softmax policy based on fast, incrementally computed, local pattern-based features consisting of both 'response' patterns around the previous move that led to state s, and 'non-response' patterns around the candidate move a in state s. Each non-response pattern is a binary feature matching a specific 3 × 3 pattern centred on a, defined by the colour (black, white, empty) and liberty count (1, 2, ≥3) for each adjacent intersection. Each response pattern is a binary feature matching the colour and liberty count in a 12-point diamond-shaped pattern 21 centred around the previous move. Additionally, a small number of handcrafted local features encode common-sense Go rules (see Extended Data Table 4). Similar to the policy network, the weights π of the rollout policy are trained from 8 million positions from human games on the Tygem server to maximize log likelihood by stochastic gradient descent. Rollouts execute at approximately 1,000 simulations per second per CPU thread on an empty board.
Our rollout policy p π (a|s) contains less handcrafted knowledge than stateof-the-art Go programs 13 . Instead, we exploit the higher-quality action selection within MCTS, which is informed both by the search tree and the policy network. We introduce a new technique that caches all moves from the search tree and then plays similar moves during rollouts; a generalization of the 'last good reply' heuristic 53 . At every step of the tree traversal, the most probable action is inserted into a hash table, along with the 3 × 3 pattern context (colour, liberty and stone counts) around both the previous move and the current move. At each step of the rollout, the pattern context is matched against the hash table; if a match is found then the stored move is played with high probability. Symmetries. In previous work, the symmetries of Go have been exploited by using rotationally and reflectionally invariant filters in the convolutional layers 24,28,29 . Although this may be effective in small neural networks, it actually hurts performance in larger networks, as it prevents the intermediate filters from identifying specific asymmetric patterns 23 . Instead, we exploit symmetries at run-time by dynamically transforming each position s using the dihedral group of eight reflections and rotations, d 1 (s), …, d 8 (s). In an explicit symmetry ensemble, a mini-batch of all 8 positions is passed into the policy network or value network and computed in parallel. For the value network, the output values are simply averaged, ( ) = ∑ ( ( ))
θ θ = v s v d s j j 1 8 1 8
. For the policy network, the planes of output probabilities are rotated/reflected back into the original orientation, and averaged together to provide an ensemble prediction, (⋅| ) = ∑ ( (⋅| ( )))

如果最大化访问次数的动作和最大化动作价值的动作相矛盾，则时间控制在其他情况下被塑造为在中盘使用最多的时间。当AlphaGo的整体评估低于预估的获胜概率10%时，即( )< -. Q s a max , 08 a ，AlphaGo投降，。AlphaGo并未采用大多数蒙特卡洛围棋程序中使用的所有行动作为第一手或快速行动价值估计启发式方法；当使用策略网络作为先验知识时，这些有偏启发式方法似乎并没有提供任何额外的好处。此外，AlphaGo也不使用逐步扩展、动态贴目或开局库。AlphaGo在与樊挥的比赛中使用的参数列在扩展数据表5中。模拟策略。模拟策略( | ) π p a s 是一种基于快速、增量计算的局部基于模式的线性softmax策略，包括了围绕导致状态s的前一步动作的“响应”模式和围绕状态s中候选动作a的“非响应”模式。每个非响应模式是一个二进制特征，匹配在a周围设定为中心的特定的3×3模式，通过每个相邻交叉点的颜色（黑、白、空）和气的计数（1、2、≥3）进行定义。每个响应模式是一个二进制特征，匹配以前一步动作为中心的12点菱形模式中的颜色和气的计数。此外，少量手工设定的局部特征编码常规围棋规则（详见扩展数据表4）。与策略网络类似，模拟策略的权重π是通过随机梯度下降法从Tygem服务器上的800万个人类对局位置中训练得到的，最大化对数似然性。模拟执行每秒大约进行1,000次的空棋盘模拟。
我们的模拟策略pπ(a|s)比最先进的围棋程序使用更少的手工设定知识。相反，我们利用MCTS中更高质量的动作选择，该选择同时受搜索树和策略网络的影响。我们引入了一种新技术，在模拟过程中缓存搜索树中的所有动作，然后在模拟中播放类似的动作；这是“最后一次好回应”启发式方法的一种推广。在树遍历的每一步中，最可能的动作会被插入到哈希表中，同时插入前一步动作和当前动作周围的3×3模式上下文（颜色、气和棋子计数）。在每一步模拟中，将模式上下文与哈希表进行匹配；如果找到匹配，则以高概率播放存储的动作。对称性。在以前的工作中，通过在卷积层中使用旋转和反射不变的滤波器来利用围棋的对称性24,28,29 。虽然这在小型神经网络中可能有效，但在大型网络中实际上会降低性能，因为它阻止了中间滤波器识别具体非对称模式23 。相反，我们通过使用八个反射和旋转的对角群d1(s)，…，d8(s)动态地变换每个位置s来在运行时利用对称性。在显式对称性集合中，将所有8个位置的mini-batch传入策略网络或价值网络并且并行计算。对于价值网络，输出值简单地取平均，( ) = ∑ ( ( ))
θ θ = v s v d s j j 1 8 1 8
。对于策略网络，输出概率平面被旋转/反射回原始方向，并且一起平均以提供一个集成预测，( ⋅| ) = ∑ ( (⋅| ( )))。

## # ARTICLE RESEARCH Part-2

σ σ = - p s d p d s j j j 1 8 18 1
; this approach was used in our raw network evaluation (see Extended Data Table 3). Instead, APV-MCTS makes use of an implicit symmetry ensemble that randomly selects a single rotation/reflection j ∈ [1, 8] for each evaluation. We compute exactly one evaluation for that orientation only; in each simulation we compute the value of leaf node s L by v θ (d j (s L )), and allow the search procedure to average over these evaluations. Similarly, we compute the policy network for a single, randomly selected rotation/reflection, ( (⋅| ( ))) Policy network: reinforcement learning. We further trained the policy network by policy gradient reinforcement learning 25,26 . Each iteration consisted of a minibatch of n games played in parallel, between the current policy network p ρ that is being trained, and an opponent ρ - p that uses parameters ρ -from a previous iter- ation, randomly sampled from a pool of opponents, so as to increase the stability of training. Weights were initialized to ρ = ρ -= σ. Every 500 iterations, we added the current parameters ρ to the opponent pool. Each game i in the mini-batch was played out until termination at step T i , and then scored to determine the outcome = ± ( ) z rs t i T i from each player's perspective. The games were then replayed to determine the policy gradient update, ∆ρ = ∑ ∑ ( -( ))
α ρ = = ∂ ( | ) ∂ ρ z v s n i n t T p a s t i t i 1 1 log i t i t i
, using the REINFORCE algorithm 25 with baseline ( ) v s t i for variance reduction. On the first pass through the training pipeline, the baseline was set to zero; on the second pass we used the value network v θ (s) as a baseline; this provided a small performance boost. The policy network was trained in this way for 10,000 minibatches of 128 games, using 50 GPUs, for one day. Value network: regression. We trained a value network ( ) ≈ ( )
θ ρ v s v s p

本翻译主要是关于将一篇学术论文准确翻译成中文，以下是翻译结果：

σ σ = - p s d p d s j j j 1 8 18 1; 这种方法在我们的原始网络评估中使用了(参见扩展数据表3)。相反，APV-MCTS利用了一个隐含的对称集合，随机选择一个旋转/反射j ∈ [1, 8]进行每次评估。我们仅计算该方向的一次评估; 在每次模拟中，通过v θ (d j (s L ))计算叶节点s L 的值，并允许搜索过程对这些评估进行平均。同样地，我们为一个单独的、随机选择的旋转/反射计算策略网络，( (⋅| ( ))) 策略网络：强化学习。我们进一步通过策略梯度强化学习进行了对策略网络的训练25,26。每次迭代由一批n个并行进行的游戏组成，游戏在当前正在训练的策略网络p ρ和对手ρ - p之间进行，对手使用来自前一个迭代的参数ρ -，从对手池中随机抽样，以提高训练的稳定性。权重初始化为ρ = ρ -= σ。每500次迭代，我们将当前参数ρ添加到对手池中。迷你批次中的每个游戏i在步骤T i 终止，然后得分以确定结果= ± ( )从每个参与者的角度来看的 z rs t i T i。然后回放游戏以确定策略梯度更新，∆ρ = ∑ ∑ ( -( ))α ρ = = ∂ ( | ) ∂ ρ z v s n i n t T p a s t i t i 1 1 log i t i t i，使用基于( ) v s t i的REINFORCE算法25进行方差减少。在第一次训练管道通过时，基线设置为零; 在第二次通过中，我们使用值网络v θ (s)作为基线；这提供了小幅度的性能提升。策略网络在这种方式下进行了训练，每个批次包含128个游戏，使用50个GPU进行了一天。价值网络：回归。我们训练了一个价值网络( ) ≈ ( )θ ρ v s v s p

## # ARTICLE RESEARCH Part-3

to approximate the value function of the RL policy network p ρ . To avoid overfitting to the strongly correlated positions within games, we constructed a new data set of uncorrelated self-play positions. This data set consisted of over 30 million positions, each drawn from a unique game of self-play. Each game was generated in three phases by randomly sampling a time step U ~ unif{1, 450}, and sampling the first t = 1,… U -1 moves from the SL policy network, a t ~ p σ (•|s t ); then sampling one move uniformly at random from available moves, a U ~ unif{1, 361} (repeatedly until a U is legal); then sampling the remaining sequence of moves until the game terminates, t = U + 1, … T, from the RL policy network, a t ~ p ρ (•|s t ). Finally, the game is scored to determine the outcome z t = ±r(s T ). Only a single training example (s U+1 , z U+1 ) is added to the data set from each game. This data provides unbiased samples of the value function
E ( )= | ~ρ + + + + ... ρ v s z s a p [ , ] p U U U U T 1 1 1 1 ,
. During the first two phases of generation we sample from noisier distributions so as to increase the diversity of the data set. The training method was identical to SL policy network training, except that the parameter update was based on mean squared error between the predicted values and the observed rewards,

为了近似RL策略网络pρ的价值函数。为了避免过度拟合在游戏中强相关的位置，我们构建了一个新的无相关的自我对弈位置数据集。这个数据集包含超过3000万个位置，每个位置都是从一场独立的自我对弈游戏中抽取的。游戏通过随机抽取一个时间步长U ~ unif{1,450}并从SL策略网络中采样前t = 1,...U-1步动作a t ~ pσ(•|s t )，然后从可用动作中随机采样一个动作a U ~ unif{1,361}（重复直到a U合法），然后从RL策略网络中采样剩下的动作序列直到游戏结束，t = U+1,...T, a t ~ pρ(•|s t )。最后，游戏被评分以确定结果z t = ±r(s T )。每场游戏只从中添加一个训练样本(s U+1 , z U+1 )到数据集中。这些数据提供了价值函数的无偏样本E( ) = | ~ρ + + + + ...ρv s z s a p[ , ] p U U U U T 1 1 1 1 。
在生成的第一个两个阶段，我们从更嘈杂的分布中采样，以增加数据集的多样性。训练方法与SL策略网络训练相同，只是参数更新基于预测值和观测奖励之间的均方误差。

## # ARTICLE RESEARCH Part-4

∆θ = ∑ ( -( )) α θ θ = ∂ ( ) ∂ θ z v s m k m k k v s 1 k
. The value network was trained for 50 million mini-batches of 32 positions, using 50 GPUs, for one week. Features for policy/value network. Each position s was pre-processed into a set of 19 × 19 feature planes. The features that we use come directly from the raw representation of the game rules, indicating the status of each intersection of the Go board: stone colour, liberties (adjacent empty points of stone's chain), captures, legality, turns since stone was played, and (for the value network only) the current colour to play. In addition, we use one simple tactical feature that computes the outcome of a ladder search 7 . All features were computed relative to the current colour to play; for example, the stone colour at each intersection was represented as either player or opponent rather than black or white. Each integer feature value is split into multiple 19 × 19 planes of binary values (one-hot encoding). For example, separate binary feature planes are used to represent whether an intersection has 1 liberty, 2 liberties,…, ≥8 liberties. The full set of feature planes are listed in Extended Data Table 2. Neural network architecture. The input to the policy network is a 19 × 19 × 48 image stack consisting of 48 feature planes. The first hidden layer zero pads the input into a 23 × 23 image, then convolves k filters of kernel size 5 × 5 with stride 1 with the input image and applies a rectifier nonlinearity. Each of the subsequent hidden layers 2 to 12 zero pads the respective previous hidden layer into a 21 × 21 image, then convolves k filters of kernel size 3 × 3 with stride 1, again followed by a rectifier nonlinearity. The final layer convolves 1 filter of kernel size 1 × 1 with stride 1, with a different bias for each position, and applies a softmax function. The match version of AlphaGo used k = 192 filters; Fig. 2b and Extended Data Table 3 additionally show the results of training with k = 128, 256 and 384 filters.

∆θ = ∑ ( -( )) α θ θ = ∂ ( ) ∂ θ z v s m k m k k v s 1 k
。该价值网络经过5000万个32个位置的mini-batches进行训练，使用50个GPU，持续一周。政策/价值网络的特征。每个位置s都进行了预处理，转化为一组19×19特征平面。我们使用的特征直接来自于游戏规则的原始表示，表示围棋棋盘每个交点的状态：棋子颜色、气（棋子链的相邻空点数）、提子次数、合法性、自上一次落子以来的回合数，以及（仅适用于价值网络）当前要落子的颜色。此外，我们还使用了一个简单的战术特征来计算连珠查找的结果。所有特征都是相对于当前要落子的颜色进行计算的；例如，每个交点的棋子颜色用玩家或对手来表示，而不是黑或白。每个整数特征值都切分成多个19×19的二值平面（独热编码）。例如，使用不同的二值特征平面表示一个交点是否有1个气、2个气，...，≥8个气。完整的特征平面集合列在扩展数据表2中。神经网络架构。政策网络的输入是一个19×19×48的图像堆栈，其中包含48个特征平面。第一个隐藏层对输入进行零填充，将其转化为一个23×23的图像，然后使用核大小为5×5、步长为1、激活函数为整流线性单元的k个滤波器与输入图像进行卷积。随后每个隐藏层2到12分别将前一个隐藏层填充为一个21×21的图像，然后使用核大小为3×3、步长为1的k个滤波器进行卷积，再次使用整流线性单元作为激活函数。最后一层使用核大小为1×1、步长为1的1个滤波器进行卷积，每个位置有不同的偏置，并应用softmax函数。AlphaGo的匹配版本使用了k = 192个滤波器；图2b和扩展数据表3还展示了k = 128、256和384个滤波器进行训练的结果。

## # ARTICLE RESEARCH Part-5

The input to the value network is also a 19 × 19 × 48 image stack, with an additional binary feature plane describing the current colour to play. Hidden layers 2 to 11 are identical to the policy network, hidden layer 12 is an additional convolution layer, hidden layer 13 convolves 1 filter of kernel size 1 × 1 with stride 1, and hidden layer 14 is a fully connected linear layer with 256 rectifier units. The output layer is a fully connected linear layer with a single tanh unit. Evaluation. We evaluated the relative strength of computer Go programs by running an internal tournament and measuring the Elo rating of each program. We estimate the probability that program a will beat program b by a logistic function Go player Fan Hui (2,908 at date of submission) 62 . All programs received a maximum of 5 s computation time per move; games were scored using Chinese rules with a komi of 7.5 points (extra points to compensate white for playing second). We also played handicap games where AlphaGo played white against existing Go programs; for these games we used a non-standard handicap system in which komi was retained but black was given additional stones on the usual handicap points. Using these rules, a handicap of K stones is equivalent to giving K -1 free moves to black, rather than K -1/2 free moves using standard no-komi handicap rules. We used these handicap rules because AlphaGo's value network was trained specifically to use a komi of 7.5.
With the exception of distributed AlphaGo, each computer Go program was executed on its own single machine, with identical specifications, using the latest available version and the best hardware configuration supported by that program (see Extended Data Table 6). In Fig. 4, approximate ranks of computer programs are based on the highest KGS rank achieved by that program; however, the KGS version may differ from the publicly available version.
The match against Fan Hui was arbitrated by an impartial referee. Five formal games and five informal games were played with 7.5 komi, no handicap, and Chinese rules. AlphaGo won these games 5-0 and 3-2 respectively (Fig. 6 and Extended Data Table 1). Time controls for formal games were 1 h main time plus three periods of 30 s byoyomi. Time controls for informal games were three periods of 30 s byoyomi. Time controls and playing conditions were chosen by Fan Hui in advance of the match; it was also agreed that the overall match outcome would be determined solely by the formal games. To approximately assess the relative rating of Fan Hui to computer Go programs, we appended the results of all ten games to our internal tournament results, ignoring differences in time controls. The policy network architecture consists of 128, or 256 filters in convolutional layers; an explicit symmetry ensemble over 2, 4 or 8 symmetries; using only the first 4, 12 or 20 input feature planes listed in Extended Data Table 1. The results consist of the test and train accuracy on the KGS data set; and the percentage of games won by given policy network against AlphaGo's policy network (highlighted row 2): using the policy networks to select moves directly (raw wins); or using AlphaGo's search to select moves (AlphaGo wins); and finally the computation time for a single evaluation of the policy network.

值网络的输入也是一个19×19×48的图像堆叠，还包括描述当前要下棋颜色的额外二进制特征平面。第2到第11个隐藏层与策略网络相同，第12个隐藏层是一个额外的卷积层，第13个隐藏层以步长为1用核大小为1×1的滤波器进行卷积，第14个隐藏层是一个具有256个修正线性单元的全连接线性层。输出层是一个具有单个tanh单元的全连接线性层。评估。我们通过进行内部比赛并测量每个程序的Elo评级来评估计算机围棋程序的相对实力。我们使用一个逻辑函数来估计程序a击败程序b的概率。所有程序每次走棋都获得最多5秒的计算时间；我们使用中国规则进行得分，考虑到先手的劫争，设定komi为7.5分。我们还进行了让子局，其中AlphaGo执白对抗现有的围棋程序；对于这些对局，我们使用了非标准的让子系统，保留了komi，但是黑方在通常的让子点上加了额外的子力。根据这些规则，k个子的让子相当于给予黑方K-1个额外的着子，而不是使用标准无劫争让子规则的K-1/2个额外着子。我们使用了这些让子规则，因为AlphaGo的价值网络是专门训练用来使用7.5的komi值的。
除去分布式AlphaGo外，每个计算机围棋程序都在自己的单独机器上执行，具有相同的规格及其程序支持的最新版本和最佳硬件配置（见扩展数据表6）。在图4中，计算机程序的近似等级基于该程序所达到的最高KGS等级；然而，KGS版本可能与公开版本不同。
与范辉的比赛由一个公正的裁判仲裁。进行了五场正式比赛和五场非正式比赛，使用7.5的komi，无让子，以及中国规则。AlphaGo分别以5-0和3-2赢得了这些比赛（图6和扩展数据表1）。正式比赛的时间控制为1小时主时间加上三个30秒的秒读时间。非正式比赛的时间控制为三个30秒的秒读时间。时间控制和比赛条件由范辉在比赛之前确定；还约定，整个比赛结果仅由正式比赛决定。为了大约评估范辉与计算机围棋程序的相对等级，我们将所有十局比赛的结果添加到我们的内部比赛结果中，忽略了时间控制的差异。策略网络的架构包括128个或256个卷积层的滤波器；在2个、4个或8个对称性上进行明确的对称性合奏；只使用扩展数据表1中列出的前4个、12个或20个输入特征平面。结果包括在KGS数据集上的测试和训练准确率；以及给定策略网络对抗AlphaGo策略网络的胜率（突出显示的第2行）：直接使用策略网络选择着子（原始胜利）；或使用AlphaGo的搜索选择着子（AlphaGo胜利）；最后是策略网络单次评估的计算时间。

## # Extended Data Table 4 | Input features for rollout and tree policy

Features used by the rollout policy (first set) and tree policy (first and second set). Patterns are based on stone colour (black/white/empty) and liberties (1, 2, ≥3) at each intersection of the pattern. 

为了实施展开策略（第一组）和树策略（第一组和第二组），采用以下输入特征。模式基于每个交叉点的棋子颜色（黑/白/空）和气（1，2，≥3）。

## # 

Supplementary Information is available in the online version of the paper. 

补充信息可在论文的在线版本中获取。

## # ARTICLE RESEARCH

Evaluating positions using rollouts only (αrp, αr), value nets only (αvp, αv), or mixing both (αrvp, αrv); either using the policy network pσ(αrvp, αvp, αrp), or no policy network (αrvp, αvp, αrp), that is, instead using the placeholder probabilities from the tree policy pτ throughout. Each program used 5 s per move on a single machine with 48 CPUs and 8 GPUs. Elo ratings were computed by BayesElo.

仅使用rollouts（αrp，αr），仅使用value nets（αvp，αv），或混合使用两者（αrvp，αrv）来评估位置；可以使用策略网络pσ（αrvp，αvp，αrp），或不使用策略网络（αrvp，αvp，αrp），而是使用树策略pτ中的占位概率。每个程序在一台拥有48个CPU和8个GPU的单机上每步使用5秒。Elo评级是通过BayesElo计算的。

## # ARTICLE RESEARCH

Extended Data 

扩展数据

